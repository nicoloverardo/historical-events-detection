{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "wikievents.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0gTFjXqBC3wq"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp1C-QrsC6Ug"
      },
      "source": [
        "# Event detection from Wikipedia pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdGlzS0yB-oD",
        "outputId": "0649fccb-2c4b-4274-f6d2-079bb94a724a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "!pip install pandarallel sparql-client p_tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "Requirement already satisfied: sparql-client in /usr/local/lib/python3.6/dist-packages (3.8)\n",
            "Requirement already satisfied: p_tqdm in /usr/local/lib/python3.6/dist-packages (1.3.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from pandarallel) (0.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sparql-client) (1.15.0)\n",
            "Requirement already satisfied: dnspython<2.0.0 in /usr/local/lib/python3.6/dist-packages (from sparql-client) (1.16.0)\n",
            "Requirement already satisfied: eventlet in /usr/local/lib/python3.6/dist-packages (from sparql-client) (0.28.0)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.6/dist-packages (from p_tqdm) (0.2.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from p_tqdm) (4.41.1)\n",
            "Requirement already satisfied: monotonic>=1.4 in /usr/local/lib/python3.6/dist-packages (from eventlet->sparql-client) (1.5)\n",
            "Requirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.6/dist-packages (from eventlet->sparql-client) (0.4.17)\n",
            "Requirement already satisfied: multiprocess>=0.70.10 in /usr/local/lib/python3.6/dist-packages (from pathos->p_tqdm) (0.70.10)\n",
            "Requirement already satisfied: ppft>=1.6.6.2 in /usr/local/lib/python3.6/dist-packages (from pathos->p_tqdm) (1.6.6.2)\n",
            "Requirement already satisfied: pox>=0.2.8 in /usr/local/lib/python3.6/dist-packages (from pathos->p_tqdm) (0.2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdCOiPH8mrJM",
        "outputId": "91b3614e-1670-41c7-9b2e-08b40a8f2bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVgX29unmrJQ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/mhc/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7nCKWEJmrJU",
        "outputId": "4056df68-b411-4982-f9be-ebebc9db01aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/mhc/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/mhc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_DZiM6HBUUY",
        "outputId": "a480db89-a651-423a-aa77-6520272926ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63ySqF6tA-R7",
        "outputId": "04bd9a43-167a-4879-ec9e-81bd7601329b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from text import PreProcessing\n",
        "from buildGloveVocab import GloveVocab\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "from tqdm import tqdm\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from p_tqdm import p_map\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 2 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTFjXqBC3wq"
      },
      "source": [
        "## Download Wikipedia pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKUq-TigCkMi"
      },
      "source": [
        "from wikiapi import WikiWrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV55Dj40A-R_"
      },
      "source": [
        "q_hist = (\n",
        "    \"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    SELECT DISTINCT ?Event_1\n",
        "    WHERE { ?Event_1 a dbo:Event .\n",
        "            ?Event_1 a dbo:MilitaryConflict . }\n",
        "\n",
        "    LIMIT 1000\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "q_nonhist_1 = (\n",
        "    \"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    SELECT DISTINCT ?Artist_1\n",
        "    WHERE { ?Artist_1 a dbo:Artist . }\n",
        "\n",
        "    LIMIT 500\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "q_nonhist_2 = (\n",
        "    \"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    SELECT DISTINCT ?Animal_1\n",
        "    WHERE { ?Animal_1 a dbo:Animal . }\n",
        "\n",
        "    LIMIT 500\n",
        "    \"\"\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "que8jlOcA-SC"
      },
      "source": [
        "hist_events_name = WikiWrapper.download_pages_name(q_hist)\n",
        "non_hist_events_name = WikiWrapper.download_pages_name(q_nonhist_1)\n",
        "non_hist_events_name = non_hist_events_name + WikiWrapper.download_pages_name(q_nonhist_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WebbplYTA-SF"
      },
      "source": [
        "df = pd.DataFrame(hist_events_name, columns=[\"Name\"])\n",
        "df[\"Abstract\"] = \"\"\n",
        "df[\"Label\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXW2Uf_LA-SJ"
      },
      "source": [
        "df1 = pd.DataFrame(non_hist_events_name, columns=[\"Name\"])\n",
        "df1[\"Abstract\"] = \"\"\n",
        "df1[\"Label\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yzfNGG5A-SM"
      },
      "source": [
        "df = pd.concat([df, df1])\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XvAdqttA-SP"
      },
      "source": [
        "df[\"Abstract\"] = df.parallel_apply(WikiWrapper.get_extract, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O74yk9P6A-ST"
      },
      "source": [
        "df.dropna(subset=[\"Abstract\"], inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RunTn0RLA-SW"
      },
      "source": [
        "df[\"Abstract\"] = df[\"Abstract\"].parallel_apply(lambda x: x.replace(\",\", \"\").replace(\"|\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXMV-tV_A-SZ"
      },
      "source": [
        "df.to_csv(\"data/wiki/wiki.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUmZW6S0CzJa"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxwtSmmvA-Se"
      },
      "source": [
        "df = pd.read_csv(\"data/wiki/wiki.csv\", sep=\"|\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obur-P-FA-Sh"
      },
      "source": [
        "df.Abstract = df.Abstract.parallel_apply(lambda x: PreProcessing.cleanText(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L5kXPl5A-Sk"
      },
      "source": [
        "df.drop(columns=[\"Name\"], inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOiS-42EA-Sy"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.Abstract.values, df.Label.values, test_size=0.2, random_state=42, stratify=df.Label.values)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2A8lnTDMzMe"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkZErV1eK1H3"
      },
      "source": [
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(BATCH_SIZE)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc4nY2_ORq9n"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VWBZB25fuum",
        "outputId": "3e29ef59-543d-42b8-fcc0-e4eed0ae805b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_index)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJUfrSlZkp6t"
      },
      "source": [
        "Skip the two cells below if you have already a compressed representation of the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOuakEMhfuup",
        "outputId": "135d4d45-c5f0-4afb-cf93-bbe814e0b4cf"
      },
      "source": [
        "path_to_glove_file = Path(\"wordemb/glove.840B.300d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i % 100000 == 0:\n",
        "            print('- At line {}'.format(i))\n",
        "\n",
        "        line = line.strip().split()\n",
        "\n",
        "        if len(line) != 300 + 1:\n",
        "            continue\n",
        "\n",
        "        word = line[0]\n",
        "        coefs = \" \".join(line[1:])\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- At line 0\n",
            "- At line 100000\n",
            "- At line 200000\n",
            "- At line 300000\n",
            "- At line 400000\n",
            "- At line 500000\n",
            "- At line 600000\n",
            "- At line 700000\n",
            "- At line 800000\n",
            "- At line 900000\n",
            "- At line 1000000\n",
            "- At line 1100000\n",
            "- At line 1200000\n",
            "- At line 1300000\n",
            "- At line 1400000\n",
            "- At line 1500000\n",
            "- At line 1600000\n",
            "- At line 1700000\n",
            "- At line 1800000\n",
            "- At line 1900000\n",
            "- At line 2000000\n",
            "- At line 2100000\n",
            "Found 2195876 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6FpZzN1fuus",
        "outputId": "260bd02f-033a-4039-d489-062e4a88257b"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embeddings = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embeddings[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "np.savez_compressed(\"data/wiki/glove_wiki.npz\", embeddings=embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:00<00:00, 216934.23it/s]\n",
            "Converted 15502 words (4498 misses)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BzvnhxJkyza"
      },
      "source": [
        "Reload your embedding matrix here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFycUQwfuux"
      },
      "source": [
        "embeddings = np.load(\"data/wiki/glove_wiki.npz\")['embeddings']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxO1zf6QRs1o"
      },
      "source": [
        "X_train = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_val = vectorizer(np.array([[s] for s in X_val])).numpy()\n",
        "X_test = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Izxrj9gA-S8"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
        "    tf.keras.layers.Embedding(len(voc) + 2, 300, embeddings_initializer=tf.keras.initializers.Constant(embeddings), trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihbgCCI0Nr0Q",
        "outputId": "e15c17ec-0d2d-494b-946a-d6f1a7085db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 300)         6000600   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 256)         439296    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,867,161\n",
            "Trainable params: 866,561\n",
            "Non-trainable params: 6,000,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtFYVvYZA-S_",
        "outputId": "87b6479b-8221-4309-86d4-1d562b33075f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        }
      },
      "source": [
        "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(X_val, y_val))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "18/18 [==============================] - 5s 266ms/step - loss: 0.2478 - accuracy: 0.8795 - val_loss: 0.0811 - val_accuracy: 0.9477\n",
            "Epoch 2/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0736 - accuracy: 0.9694 - val_loss: 0.0987 - val_accuracy: 0.9617\n",
            "Epoch 3/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0591 - accuracy: 0.9555 - val_loss: 0.0680 - val_accuracy: 0.9721\n",
            "Epoch 4/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0516 - accuracy: 0.9668 - val_loss: 0.0692 - val_accuracy: 0.9721\n",
            "Epoch 5/20\n",
            "18/18 [==============================] - 3s 179ms/step - loss: 0.0531 - accuracy: 0.9598 - val_loss: 0.0723 - val_accuracy: 0.9721\n",
            "Epoch 6/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0517 - accuracy: 0.9642 - val_loss: 0.0694 - val_accuracy: 0.9721\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0455 - accuracy: 0.9721 - val_loss: 0.0686 - val_accuracy: 0.9721\n",
            "Epoch 8/20\n",
            "18/18 [==============================] - 3s 182ms/step - loss: 0.0462 - accuracy: 0.9738 - val_loss: 0.0705 - val_accuracy: 0.9721\n",
            "Epoch 9/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0445 - accuracy: 0.9703 - val_loss: 0.0717 - val_accuracy: 0.9721\n",
            "Epoch 10/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0456 - accuracy: 0.9686 - val_loss: 0.0727 - val_accuracy: 0.9721\n",
            "Epoch 11/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0456 - accuracy: 0.9677 - val_loss: 0.0738 - val_accuracy: 0.9721\n",
            "Epoch 12/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0457 - accuracy: 0.9677 - val_loss: 0.0738 - val_accuracy: 0.9721\n",
            "Epoch 13/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0444 - accuracy: 0.9721 - val_loss: 0.0743 - val_accuracy: 0.9721\n",
            "Epoch 14/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0457 - accuracy: 0.9686 - val_loss: 0.0749 - val_accuracy: 0.9721\n",
            "Epoch 15/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0455 - accuracy: 0.9694 - val_loss: 0.0752 - val_accuracy: 0.9721\n",
            "Epoch 16/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0451 - accuracy: 0.9668 - val_loss: 0.0755 - val_accuracy: 0.9721\n",
            "Epoch 17/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0457 - accuracy: 0.9581 - val_loss: 0.0761 - val_accuracy: 0.9512\n",
            "Epoch 18/20\n",
            "18/18 [==============================] - 3s 180ms/step - loss: 0.0451 - accuracy: 0.9677 - val_loss: 0.0766 - val_accuracy: 0.9721\n",
            "Epoch 19/20\n",
            "18/18 [==============================] - 3s 181ms/step - loss: 0.0456 - accuracy: 0.9703 - val_loss: 0.0766 - val_accuracy: 0.9721\n",
            "Epoch 20/20\n",
            "18/18 [==============================] - 3s 179ms/step - loss: 0.0459 - accuracy: 0.9581 - val_loss: 0.0769 - val_accuracy: 0.9721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO7LldMZA-TC",
        "outputId": "86dde18c-a1c0-45db-a66f-a5c37c99e39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "_, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 1s 51ms/step - loss: 0.1109 - accuracy: 0.9610\n",
            "Test Accuracy: 0.961002767086029\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "wikievents.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0gTFjXqBC3wq"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp1C-QrsC6Ug"
      },
      "source": [
        "# Event detection from Wikipedia pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdGlzS0yB-oD"
      },
      "source": [
        "!pip install pandarallel sparql-client p_tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdCOiPH8mrJM",
        "outputId": "b3efe8b0-409a-4a20-d52b-6f6bc2d75149",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVgX29unmrJQ"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks/mhc/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7nCKWEJmrJU",
        "outputId": "d80d118a-3509-4978-b83a-b269091a14bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks/mhc/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/mhc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_DZiM6HBUUY",
        "outputId": "e061b021-557d-415b-94a8-901756eb21aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63ySqF6tA-R7",
        "outputId": "9d4f5bd7-15f2-4d02-f09c-088fc9f33254",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from text import PreProcessing\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from tqdm import tqdm\n",
        "import multiprocessing as mp\n",
        "from multiprocessing import Pool\n",
        "\n",
        "from p_tqdm import p_map\n",
        "\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 2 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gTFjXqBC3wq"
      },
      "source": [
        "## Download Wikipedia pages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKUq-TigCkMi"
      },
      "source": [
        "from wikiapi import WikiWrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV55Dj40A-R_"
      },
      "source": [
        "q_hist = (\n",
        "    \"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    SELECT DISTINCT ?Event_1\n",
        "    WHERE { ?Event_1 a dbo:Event .\n",
        "            ?Event_1 a dbo:MilitaryConflict . }\n",
        "\n",
        "    LIMIT 1000\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "q_nonhist_1 = (\n",
        "    \"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    SELECT DISTINCT ?Artist_1\n",
        "    WHERE { ?Artist_1 a dbo:Artist . }\n",
        "\n",
        "    LIMIT 500\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "q_nonhist_2 = (\n",
        "    \"\"\"\n",
        "    PREFIX dbo: <http://dbpedia.org/ontology/>\n",
        "    SELECT DISTINCT ?Animal_1\n",
        "    WHERE { ?Animal_1 a dbo:Animal . }\n",
        "\n",
        "    LIMIT 500\n",
        "    \"\"\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "que8jlOcA-SC"
      },
      "source": [
        "hist_events_name = WikiWrapper.download_pages_name(q_hist)\n",
        "non_hist_events_name = WikiWrapper.download_pages_name(q_nonhist_1)\n",
        "non_hist_events_name = non_hist_events_name + WikiWrapper.download_pages_name(q_nonhist_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WebbplYTA-SF"
      },
      "source": [
        "df = pd.DataFrame(hist_events_name, columns=[\"Name\"])\n",
        "df[\"Abstract\"] = \"\"\n",
        "df[\"Label\"] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXW2Uf_LA-SJ"
      },
      "source": [
        "df1 = pd.DataFrame(non_hist_events_name, columns=[\"Name\"])\n",
        "df1[\"Abstract\"] = \"\"\n",
        "df1[\"Label\"] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yzfNGG5A-SM"
      },
      "source": [
        "df = pd.concat([df, df1])\n",
        "df.reset_index(inplace=True, drop=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XvAdqttA-SP"
      },
      "source": [
        "df[\"Abstract\"] = df.parallel_apply(WikiWrapper.get_extract, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O74yk9P6A-ST"
      },
      "source": [
        "df.dropna(subset=[\"Abstract\"], inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RunTn0RLA-SW"
      },
      "source": [
        "df[\"Abstract\"] = df[\"Abstract\"].parallel_apply(lambda x: x.replace(\",\", \"\").replace(\"|\", \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXMV-tV_A-SZ"
      },
      "source": [
        "df.to_csv(\"data/wiki/wiki.csv\", index=False, sep=\"|\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUmZW6S0CzJa"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxwtSmmvA-Se"
      },
      "source": [
        "df = pd.read_csv(\"data/wiki/wiki.csv\", sep=\"|\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obur-P-FA-Sh"
      },
      "source": [
        "df.Abstract = df.Abstract.parallel_apply(lambda x: PreProcessing.cleanText(x))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L5kXPl5A-Sk"
      },
      "source": [
        "df.drop(columns=[\"Name\"], inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOiS-42EA-Sy"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.Abstract.values,\n",
        "                                                    df.Label.values,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=df.Label.values)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train,\n",
        "                                                  y_train,\n",
        "                                                  test_size=0.2,\n",
        "                                                  random_state=42,\n",
        "                                                  stratify=y_train)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2A8lnTDMzMe"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 50"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkZErV1eK1H3"
      },
      "source": [
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(BATCH_SIZE)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc4nY2_ORq9n"
      },
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJUfrSlZkp6t"
      },
      "source": [
        "Skip the two cells below if you have already a compressed representation of the embedding matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOuakEMhfuup",
        "outputId": "135d4d45-c5f0-4afb-cf93-bbe814e0b4cf"
      },
      "source": [
        "path_to_glove_file = Path(\"wordemb/glove.840B.300d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i % 100000 == 0:\n",
        "            print('- At line {}'.format(i))\n",
        "\n",
        "        line = line.strip().split()\n",
        "\n",
        "        if len(line) != 300 + 1:\n",
        "            continue\n",
        "\n",
        "        word = line[0]\n",
        "        coefs = \" \".join(line[1:])\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "- At line 0\n",
            "- At line 100000\n",
            "- At line 200000\n",
            "- At line 300000\n",
            "- At line 400000\n",
            "- At line 500000\n",
            "- At line 600000\n",
            "- At line 700000\n",
            "- At line 800000\n",
            "- At line 900000\n",
            "- At line 1000000\n",
            "- At line 1100000\n",
            "- At line 1200000\n",
            "- At line 1300000\n",
            "- At line 1400000\n",
            "- At line 1500000\n",
            "- At line 1600000\n",
            "- At line 1700000\n",
            "- At line 1800000\n",
            "- At line 1900000\n",
            "- At line 2000000\n",
            "- At line 2100000\n",
            "Found 2195876 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6FpZzN1fuus",
        "outputId": "260bd02f-033a-4039-d489-062e4a88257b"
      },
      "source": [
        "num_tokens = len(voc) + 2\n",
        "embedding_dim = 300\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embeddings = np.zeros((num_tokens, embedding_dim))\n",
        "for word, i in tqdm(word_index.items()):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embeddings[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "\n",
        "np.savez_compressed(\"data/wiki/glove_wiki.npz\", embeddings=embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20000/20000 [00:00<00:00, 216934.23it/s]\n",
            "Converted 15502 words (4498 misses)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BzvnhxJkyza"
      },
      "source": [
        "Reload your embedding matrix here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bFycUQwfuux"
      },
      "source": [
        "embeddings = np.load(\"data/wiki/glove_wiki.npz\")['embeddings']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxO1zf6QRs1o"
      },
      "source": [
        "X_train = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_val = vectorizer(np.array([[s] for s in X_val])).numpy()\n",
        "X_test = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo45_04i66Qm"
      },
      "source": [
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Izxrj9gA-S8"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.Input(shape=(None,), dtype=\"int64\"),\n",
        "    tf.keras.layers.Embedding(len(voc) + 2, 300, embeddings_initializer=tf.keras.initializers.Constant(embeddings), trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "es = EarlyStopping(monitor='loss', verbose=1,\n",
        "                   mode='min', patience = 2, min_delta=0.01)\n",
        "\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average=\"micro\")])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihbgCCI0Nr0Q",
        "outputId": "1dde42f2-0011-4470-f3e3-f2449b5aba25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 300)         6000600   \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, None, 256)         439296    \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 6,867,161\n",
            "Trainable params: 866,561\n",
            "Non-trainable params: 6,000,600\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtFYVvYZA-S_",
        "outputId": "81154826-131f-4519-d700-e665a2b2c073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=[es])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "18/18 [==============================] - 2s 131ms/step - loss: 0.5942 - accuracy: 0.8341 - f1_score: 0.6337 - val_loss: 0.5357 - val_accuracy: 0.9686 - val_f1_score: 0.6333\n",
            "Epoch 2/50\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.5345 - accuracy: 0.9712 - f1_score: 0.6337 - val_loss: 0.5422 - val_accuracy: 0.9582 - val_f1_score: 0.6333\n",
            "Epoch 3/50\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.5361 - accuracy: 0.9686 - f1_score: 0.6337 - val_loss: 0.5400 - val_accuracy: 0.9617 - val_f1_score: 0.6333\n",
            "Epoch 4/50\n",
            "18/18 [==============================] - 1s 57ms/step - loss: 0.5364 - accuracy: 0.9677 - f1_score: 0.6337 - val_loss: 0.5335 - val_accuracy: 0.9721 - val_f1_score: 0.6333\n",
            "Epoch 00004: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QO7LldMZA-TC",
        "outputId": "880b78ac-ffc7-4dd9-fd76-68ee6ea7cc9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5428 - accuracy: 0.9526 - f1_score: 0.6324\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5428334474563599, 0.9526462554931641, 0.6323809623718262]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF5RgQeq44Zh",
        "outputId": "d90b4f9d-d0cf-46b4-9889-5ddf66f2d6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(classification_report(y_test, (model.predict(X_test) > 0.5).astype(\"int32\")))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.96       193\n",
            "           1       0.94      0.96      0.95       166\n",
            "\n",
            "    accuracy                           0.95       359\n",
            "   macro avg       0.95      0.95      0.95       359\n",
            "weighted avg       0.95      0.95      0.95       359\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}